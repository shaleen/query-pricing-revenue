\section{Introduction}
\label{sec:intro}

The last decade or so has seen an explosion of data being collected from a variety of sources and across a broad range of areas. Many companies, including Bloomberg~\cite{bloomberg}, Twitter~\cite{twitterapi}, Lattice Data~\cite{lattice}, DataFinder~\cite{datafinder}, and Banjo~\cite{banjo} collect such data, which they then sell as structured (relational) datasets. 
These datasets are also often sold through online {\em data markets}, which are web platforms for buying and selling data: examples include BDEX~\cite{bdex}, Salesforce~\cite{salesforce} and QLik DataMarket~\cite{qlik}. Even though data sellers and data markets offer an abundance of data products, the pricing schemes currently used are very simplistic. In most cases, a data buyer has only one option, to buy the whole dataset (or a bundle of datasets) at a fixed price. Alternatively, the dataset is split into multiple disjoint chunks, and each chunk is sold at a separate price. 

However, data buyers are commonly interested in extracting specific information from a dataset and not in acquiring the whole dataset. Accessing this information can often be concisely captured through a {\em query}, or a sequence of queries. Selling the whole dataset at a fixed price forces the buyer to either pay more for the query than it is valued, or to choose not to access it. This means that valuable data is often not accessible to lay users, scientists, or entities with limited budgets, and moreover that data-selling companies and marketplaces behave suboptimally with respect to maximizing their revenue.

To address this problem, a recent line of research~\cite{KUBHS12,KUBHS13,deep2017qirana} in the database community introduced the framework of query-based pricing. A {\em query-based pricing scheme} tailors the purchase of the data to the user's needs, by assigning a price to each query issued over the dataset. Given a dataset $\db$ and a query $Q$ over the dataset, the user must pay a price $p(Q,\db)$ to obtain the answer $Q(\db)$ of the query. This price reflects only the value of the information learned by obtaining the query answer, and not the computational cost of executing the query. The work on query-based pricing has mainly focused on how to define a well-behaved pricing function, and how to develop system support for efficiently implementing a data marketplace.
In particular, a key property that a pricing function must obey is that of {\em arbitrage freeness}: it should not be possible for the buyer to acquire a query for a cheaper price through the combination of other query results. This is a type of incentive constraint. The arbitrage-freeness constraint makes the design of appropriate pricing functions a challenging task, especially since deciding whether a query is more informative than another query (or set of queries) is generally a computationally hard problem, and for practical applications it is critical that the price computation can be performed efficiently.

To overcome this computational barrier, \citet{deep2017qirana} proposed a setup where the seller and buyers have a common knowledge base $\mI$ consisting of multiple potential datasets. Each query (or set of queries) can then be thought of as a function that classifies datasets. The answer to a query identifies the collection of datasets in $\mI$ that are consistent with that answer. Whether a query is more informative than another then amounts to whether it identifies more inconsistent datasets than the latter. The benefit of this restricted model is that the query pricing can now be cast as a problem of pricing subsets over a ground set of items, each item being a dataset in the knowledge base $\mI$. The arbitrage-freeness constraint corresponds to the pricing function being monotone and subadditive. Of course, finding the optimal subadditive pricing is also a computationally hard problem. Furthermore, a general subadditive function, even if we manage to find one, can take exponential space to store. We therefore ask whether it is possible to find a simple pricing function that approximates the optimal subadditive pricing in terms of the seller's revenue. Observe that in the query pricing setting, there is no cost to the seller for replicating answers to queries, and so we can model the seller as having {\em unlimited supply} for each item. We further assume that buyers are single minded---each buyer wants to buy the answer to a single query.

% To overcome this computational barrier, one possible solution proposed in~\cite{deep2017qirana}  is to model each query $Q$ as a {\em bundle} of items $B(Q)$ from a common itemset $I$. Then, the arbitrage-free constraint translates to the requirement that the pricing function must be {\em monotone} and {\em subadditive} when viewed as a set function. Among such set functions, of particular practical interest are the additive and constant functions. An additive function gives to each item $i \in I$  a weight $w_i \geq 0$, and assigns the price $p(Q,\db) = \sum_{i \in B(Q)} w_i$, while a constant function simply assigns the same  price to every query, \ie $p(Q,\db) = p$.
% However, prior work has not answered the fundamental question of how one can choose among the possible set functions (or subclasses of them) the one that maximizes the revenue of the seller. 

Revenue maximization with unlimited supply and single-minded buyers has been studied extensively from a theoretical perspective and a number of approximation algorithms are known (see, e.g., \cite{guruswami2005profit,balcan2006approximation,briest2006single}). We discuss these algorithms in detail below. These algorithms produce pricings with worst case approximation factors logarithmic in one or more of the natural parameters of the instance, and these results are known to be tight to within constant factors in the worst case. But how well do these approximation results hold up in practice? Does the seller really need to give up on all but a logarithmic fraction of the optimal revenue in order to gain computational efficiency? Which of these algorithms should a practitioner use, and what features of the problem instance dictate this choice? These are some of the questions we study in this paper.


% This literature aims to obtain worst case approximation guarantees for revenue using additive pricing functions, a.k.a. item pricing. An item pricing gives to each item $i$  a weight $w_i \geq 0$, and assigns the price $p(S) = \sum_{i \in S} w_i$ to a bundle $S$ of items. There are a number of different algorithms and corresponding analyses in literature that produce item pricings with worst case approximation factors logarithmic in one or more of the natural parameters of the instance---the number of items $n$, the number of bundles $m$, the size of the largest bundle $k$, and the maximum number of bundles any item belongs to $B$. Many of these analyses are tight to within constant factors in the worst case.

\vspace{1em}
\noindent
{\em The goal of this work is to investigate how well theoretical performance bounds for revenue maximization with unlimited supply hold up in practice, using the setting of query pricing for data markets as an application.} 
\vspace{1em}

We perform an experimental study to evaluate the quality of various pricing algorithms for different problem instances, both synthetic and real. The approximation ratio of a pricing algorithm is just one of many features that determine how practical the algorithm is. We compare algorithms in terms of the revenue obtained, as well as their running time and the representation complexity of the pricing produced. Enroute we develop a new algorithm and a post-processing heuristic to use in conjunction with known algorithms to improve performance, and prove some new performance bounds. We now discuss these contributions in more detail.

We study three kinds of pricings. The first, {\em uniform bundle pricing}, assigns the same price to every bundle and is the default pricing scheme in many data markets currently. The second, additive or {\em item pricing}, assigns a price to each item and charges a price for each bundle equal to the sum of prices for the items in the bundle. Most approximation results known produce item pricings. Third, we consider a much more general class of pricings, namely {\em XOS or fractionally subadditive pricings}. These pricings are much more expressive than item or uniform bundle pricings, while at the same time having small representation size. We show that XOS pricings can achieve a logarithmic factor larger revenue than the better of the latter two. However, there are no algorithms known to exploit the benefits of this class of pricings.

We study the performance of the following pricing algorithms. In quantifying performance, several parameters of the instance are relevant: the number of items $n$, the number of bundles $m$, the size of the largest bundle $k$, and the maximum number of bundles any item belongs to $B$. In the query pricing setting it is usually the case that $B\le m\ll k\le n$, so algorithms with approximation factors and running time depending on $B$ or $m$ are generally better than those depending on $k$ or $n$.
\begin{itemize}
\item {\bf Capacity-based item pricing.} This algorithm due to Cheung and Swamy~\cite{cheung2008approximation} achieves the best known worst case approximation ratio for revenue, namely $O(\log B)$. This ratio is tight. The algorithm involves solving multiple linear programs, each with $O(n)$ variables and constraints, for each instance of the problem and is therefore slow in practice.
\item {\bf Uniform item pricing.} This simple algorithm prices every item at the same amount, and achieves an approximation ratio of $O(\log n+\log m)$. 
\item {\bf LP-based post processing.} Given any item pricing, and the subset of bundles that get purchased by the corresponding buyers at those prices, we can ask whether it is possible to increase prices while selling the same bundles so as to extract more revenue. This optimization can be expressed as a linear program and solved efficiently. We apply this optimization as a post processing step to the above two algorithms achieving significant performance gains in experiments, although it does not improve the worst case approximation ratio.
\item {\bf Layering based item pricing.} We develop a new greedy algorithm that partitions bundles into groups, each of which can be priced optimally to extract their entire value as revenue. We show that this algorithm achieves an $O(B)$ approximation in the worst case. Although this approximation factor is exponentially worse than that of the first algorithm, we observe that for many workloads the two algorithms achieve similar performance, and the simplicity and speed of the greedy algorithm gives it a distinct advantage. 
\item {\bf Uniform bundle pricing.} Finally, For the sake of comparison, we also implement and report results for the best uniform bundle pricing, namely one that assigns the same price to every bundle.
\end{itemize}


% We study the performance of three different pricing algorithms in this paper. The first is an item pricing algorithm due to Cheung and Swamy~\cite{cheung2008approximation} that achieves the best known worst case approximation ratio for revenue, namely $O(\log B)$. This algorithm involves solving multiple linear programs for each instance of the problem, and is therefore very slow in practice. The second algorithm we consider is an item pricing with a worse approximation ratio of $O(\log n+\log m)$. This is a simple algorithm that produces a uniform item pricing, namely one where all item weights, $w_i$, are equal. The simplicity of the algorithm allows for algorithmic improvements that do not hurt its approximation ratio but for some instances can greatly improve performance. We describe one such improvement in Section~\ref{section-approxalgo}. The third algorithm we study is an extremely fast greedy item pricing algorithm. We show in Section~\ref{section-approxalgo} that the greedy algorithm achieves a worst case approximation factor of $O(B)$. Although this approximation factor is exponentially worse than that of the first algorithm, we observe that for many workloads the two algorithms achieve similar performance, and the simplicity and speed of the greedy algorithm gives it a distinct advantage. For the sake of comparison, we also implement and report results for a constant pricing function, namely one that assigns the same price to every bundle (query). \todo{Say something about XOS?}

Our experimental study shows that the worst-case analysis of pricing algorithms does not capture how well the algorithms behave in real-world instances in terms of approximating the optimal revenue, for the specific application of query-based pricing. In particular, we observe that the structure of the bundles induced by different query workloads heavily influences the quality of approximation. For example, despite achieving the best known worst case approximation ratio, capacity-based item pricing did not achieve the best performance of the algorithms we tested in even one of our experimental setups. Our work suggests that a promising direction is to attempt to obtain better approximation guarantees by taking into account the structure of the bundles, and also the type of valuations that occur in real-world settings.

% In this paper, we tackle the above question building on ideas from the optimal pricing 
% literature~\cite{guruswami2005profit}. We consider the {\em unlimited supply} setting, where the 
% seller can sell any number of units of each query. This is a natural assumption in the context of a data market,
% since multiple buyers can request and purchase the same query.
% Additionally, we assume that the buyers are {\em single-minded}, so each
% buyer is interested in buying only a single query $Q$ (so a single bundle) for a price of $v_Q$; the buyer will
% purchase the query only if the price $p(Q, \db)$ does not exceed $v_Q$. We focus on the aforementioned two
% types of pricing functions that either assign a uniform price to every query, or perform item pricing. 
% In the case of item pricing, the problem of computing the revenue-maximizing prices can be cast as
% the well-studied {\em hypergraph vertex pricing} problem, where only approximation guarantees are known.










